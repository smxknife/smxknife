## K最邻近算法(k-nearest neighbours，KNN)

### 学习内容

* 学习使用K最近邻算法创建分类系统
* 学习特征抽取
* 学习回归，即预测数值
* K最邻近算法的案例及局限性

### K最邻近算法

什么是k最邻近算法？举一个例子，柚子和橙子，柚子通常比橙子大和红，那么下面这个点的水果是柚子还是橙子？
![](https://i.loli.net/2019/05/06/5ccfc3286ba0f.jpg)
这种情况，可以采用观察离它最近的三个邻居，如下
![](https://i.loli.net/2019/05/06/5ccfc3c512fd9.jpg)
根据上图，橙子比柚子多，因此有可能是橙子。

上面这个例子使用的就是k最邻近算法进行分类

### 特征抽取

上面的柚子橙子分类算法很简单，有一个前提，红和大这两个特征是已经明确的前提，所以，使用k最邻近算法，还要先进行特征抽取

采用毕达哥拉斯定理来求相似程度

例如两个点：a1(x1, y1, z1, t1) a2(x2, y2, z2, t2), 那么这两个点的距离可以用毕达哥拉斯定理解释为

    sqrt( (x1 - x2)^2 + (y1 - y2)^2 + (z1 - z2)^2 + (t1 - t2)^2 ) 
    
### 回归

特征值抽取了，分类也分了，可以利用分类进行推荐系统或其他的操作，那么，同样可以利用分组进行回归预测。

选取一定数量的邻居，然后对其求平均值，这就是一个简单的回归预测

### knn算法的用途

使用knn算法基本上就是做两件事：
* 分类编组
* 回归预测

### 余弦相似度

在上面的特征抽取时采用毕达哥拉斯定理获取距离来进行分组，而在实际的工作中会经常采用**余弦相似度**

余弦相似度不是计算两点的距离，而是比较他们的角度。
> 0度角的余弦值是1，而其他任何角度的余弦值不大于1，并且最小值是-1，直角时余弦值为0

利用上面这个特性来判断两个向量的大致指向方向是否相同，这种结果与长度无关，仅仅与向量的指向有关，通常用于正空间，因此给出的值为0到1

### 机器学习

OCR：光学字符识别，虽然特征提起比较复杂，但是原理依然是knn算法

垃圾邮件过滤器：使用"朴素贝叶斯分类器"
